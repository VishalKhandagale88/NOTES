## 2. Embeddings (turning tokens into meaning)

Once we have tokens, we still need to make them **understandable for a computer**.

Computers donâ€™t â€œgetâ€ words, they get **numbers**.

So, we **turn each token into a list of numbers** (a vector).

That list captures the **meaning** of the word.
- ### Example:
- â€œkingâ€ â†’ [0.21, 0.87, -0.44, â€¦]
- â€œqueenâ€ â†’ [0.19, 0.85, -0.40, â€¦]
  
  Notice how **â€œkingâ€ and â€œqueenâ€ end up with numbers that are close together** â€” because they have related meanings.
- ## ğŸ§  Why embeddings are powerful
  
  Embeddings let AI **understand relationships** between words:
- â€œking - man + woman â‰ˆ queenâ€
- â€œParis is to Franceâ€ like â€œTokyo is to Japanâ€
  
  Itâ€™s like mapping every word into a giant **3D map of meaning**, where similar words are near each other.