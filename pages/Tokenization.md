- [[Byte Pair Encoding]]
- ## 1. Tokenization (breaking text into pieces)
  
  Imagine you have a big sentence:
  
  **â€œI love chocolate ice cream.â€**
  
  A computer canâ€™t understand the whole sentence the way humans do.
  
  So first, we **break it down into smaller chunks** called **tokens**.
- ### Different ways to tokenize:
- **By words:**
  
  â†’ [â€œIâ€, â€œloveâ€, â€œchocolateâ€, â€œiceâ€, â€œcreamâ€]
- **By sub-words (common in AI models):**
  
  â†’ [â€œIâ€, â€œloveâ€, â€œchocoâ€, â€œlateâ€, â€œiceâ€, â€œcreamâ€]
- **By characters:**
  
  â†’ [â€œIâ€, â€œlâ€, â€œoâ€, â€œvâ€, â€œeâ€, â€¦]
  
  Think of **tokens as Lego blocks ğŸ§±** that we use to build and understand language.
-
- After tokenization [[Embeddings]] comes into play
-